{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCdp93DnRYtF"
   },
   "source": [
    "# **Training and testing gender detector model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gp7V6DaxRtGf"
   },
   "source": [
    "Mount google drive with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "1zft5nQlmIhq",
    "outputId": "bea36c37-724d-4a29-812f-714a4d6e7c0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_2KkNS2R3N_"
   },
   "source": [
    "Unpacking the dataset archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/My Drive/internship_data_cleaned.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNFWnR27x3qu"
   },
   "source": [
    "Analysis of the dataset revealed the following problems:\n",
    "- presence of duplicates\n",
    "- class mismatch\n",
    "- lack of informative signs\n",
    "- impossibility of visual identification\n",
    "\n",
    "The FSlint program was used to remove duplicates.\n",
    "The rest of the problems were partially solved in manual mode.\n",
    "To fully solve these problems, it is necessary to use machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "colab_type": "code",
    "id": "TXxYFfh-9xQ_",
    "outputId": "5507fd83-b70b-413a-cf40-440f735ce9b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_count: 89679 \n",
      "valid_count: 4982\n",
      "test_count:  4983\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "data_path = 'internship_data/'\n",
    "total_count = len(glob(data_path + '*/*'))\n",
    "batch = 16 # Experiments have shown that a small batch is more preferable\n",
    "workers = 4\n",
    "\n",
    "# Divide the dataset in ration 0.9/0.05/0.05. \n",
    "# We need a validation dataset to evaluate overfitting\n",
    "train_count = int(0.9 * total_count)\n",
    "valid_count = int(0.05 * total_count)\n",
    "test_count = total_count - train_count - valid_count\n",
    "print('''train_count: {} \n",
    "valid_count: {}\n",
    "test_count:  {}'''.format(train_count, valid_count, test_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VvUhLzle7VZ"
   },
   "source": [
    "For validation and testing we will resize and normalize dataset. Large image size is not required therefore we use a 128 by 128 image. For training we will use additional transformations. Random rotation and horizontal flip will diversify training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "blEecdre9xRP",
    "outputId": "5c9d5544-6e0d-4d1a-f3df-4137a68fbdaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "use_cuda:  True\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch import cuda\n",
    "\n",
    "# Class for applying multiple transforms\n",
    "class MapDataset(Dataset):\n",
    "    def __init__(self, dataset, map_fn):\n",
    "        self.dataset = dataset\n",
    "        self.map = map_fn\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.map:     \n",
    "            x = self.map(self.dataset[index][0]) \n",
    "        else:     \n",
    "            x = self.dataset[index][0]  # image\n",
    "        y = self.dataset[index][1]   # label      \n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "data_transforms = {'train': transforms.Compose([\n",
    "                            transforms.Resize([128, 128]),\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                            transforms.RandomRotation(degrees=15),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                 std=[0.229, 0.224, 0.225])]),\n",
    "                   'test':  transforms.Compose([\n",
    "                            transforms.Resize([128, 128]),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                 std=[0.229, 0.224, 0.225])])}\n",
    "\n",
    "# Create dataset without transforms\n",
    "dataset = ImageFolder(data_path)\n",
    "\n",
    "# Split to train, validation and test\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, (train_count, \n",
    "                                                                    valid_count, \n",
    "                                                                    test_count))\n",
    "\n",
    "# Mapping transforms to the corresponding datasets \n",
    "train_dataset = MapDataset(train_dataset, data_transforms['train'])\n",
    "valid_dataset = MapDataset(valid_dataset, data_transforms['test'])\n",
    "test_dataset = MapDataset(test_dataset, data_transforms['test'])\n",
    "\n",
    "# Create three dataloaders. We don't need to shuffle the test dataset\n",
    "train_dataset_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True, \n",
    "                                  num_workers=workers)  \n",
    "valid_dataset_loader = DataLoader(valid_dataset, batch_size=batch, shuffle=True, \n",
    "                                  num_workers=workers) \n",
    "test_dataset_loader  = DataLoader(test_dataset, batch_size=batch, shuffle=False,\n",
    "                                  num_workers=workers)\n",
    "# Dict of dataloaders\n",
    "dataloaders = {'train': train_dataset_loader, \n",
    "               'valid': valid_dataset_loader, \n",
    "               'test': test_dataset_loader}\n",
    "\n",
    "# Use of GPU for fast learning\n",
    "use_cuda = cuda.is_available()\n",
    "print('\\nuse_cuda: ', use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUcyghfdjMkK"
   },
   "source": [
    "The assigned task is a classification task. We will use CNN architecture. Since it is necessary to define only two classes, max poolling layers will reduce the number of parameters without unnecessary loss of information. Using Dropout layers will reduce the impact of overfitting during long training. At the output, we use the log_softmax function to apply the NLLLoss loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DqgeM8K67GX2"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module, Conv2d, MaxPool2d, Linear, Dropout\n",
    "from torch.nn.functional import relu, log_softmax\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 128x128x3\n",
    "        self.conv1 = Conv2d(3, 32, kernel_size=3, stride=1, padding=0)\n",
    "        # 126x126x32\n",
    "        self.conv2 = Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        # 124x124x32\n",
    "        self.max_pool1 = MaxPool2d(2, 2)\n",
    "        \n",
    "        # 62x62x32\n",
    "        self.conv3 = Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        # 60x60x32\n",
    "        self.conv4 = Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        # 58x58x32\n",
    "        self.max_pool2 = MaxPool2d(2, 2)\n",
    "         \n",
    "        # 29x29x32\n",
    "        self.conv5 = Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "        # 27x27x64\n",
    "        self.conv6 = Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "        # 25x25x64\n",
    "        self.max_pool3 = MaxPool2d(2, 2)\n",
    "\n",
    "        # 12x12x64\n",
    "        self.conv7 = Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "        # 10x10x64\n",
    "        self.conv8 = Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "        # 8x8x64\n",
    "        self.max_pool4 = MaxPool2d(2, 2)\n",
    "\n",
    "        # 4x4x64\n",
    "        self.fc1 = Linear(4*4*64, 512)        \n",
    "        self.drop = Dropout(0.2)\n",
    "        self.fc2 = Linear(512, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define forward behavior\n",
    "        x = relu(self.conv1(x))\n",
    "        x = relu(self.conv2(x))\n",
    "        x = self.max_pool1(x)\n",
    "            \n",
    "        x = relu(self.conv3(x))\n",
    "        x = relu(self.conv4(x))\n",
    "        x = self.max_pool2(x)\n",
    "\n",
    "        x = relu(self.conv5(x))\n",
    "        x = relu(self.conv6(x))\n",
    "        x = self.max_pool3(x)\n",
    "\n",
    "        x = relu(self.conv7(x))\n",
    "        x = relu(self.conv8(x))\n",
    "        x = self.max_pool4(x)\n",
    "             \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)        \n",
    "        x = self.fc2(x)\n",
    "        x = log_softmax(x, -1)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PC4-o-9oSD6I"
   },
   "source": [
    "CNN model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "colab_type": "code",
    "id": "-kz2GREO8zIP",
    "outputId": "de6b08fd-e454-4967-c37f-912876ae012f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5B8aE8ekAmG"
   },
   "source": [
    "After several experiments it was found that NLLLoss is more suitable than CrossEntropy function for this problem. It gives a more accuracy. Adam was chosen as the learning algorithm. it's stable and does a fairly good job of finding a more or less optimal solution. experiments have shown that the learning rate 2e-4 is optimal for getting out of local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKE1O-6-9xRz"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import NLLLoss\n",
    "\n",
    "criterion = NLLLoss()\n",
    "optimizer = Adam(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "697t_Rc4SW5w"
   },
   "source": [
    "Write the training and validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QsQLsIGC9xSG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import save\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # Initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # Initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # Move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Calculate the loss\n",
    "            loss = criterion(model(data), target)\n",
    "            # Compute gradient of the loss\n",
    "            loss.backward()\n",
    "            # Parameters update\n",
    "            optimizer.step()\n",
    "            # Update running training loss\n",
    "            train_loss += (loss.data - train_loss) / (batch_idx + 1)\n",
    "            # Print loss every 100 epochs\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('train_loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        # Average training loss\n",
    "        train_loss /= len(loaders['train'].dataset) \n",
    "          \n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # Move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # Calculate the loss\n",
    "            loss = criterion(model(data), target)\n",
    "            \n",
    "            valid_loss += (loss.data - valid_loss) / (batch_idx + 1)\n",
    "            \n",
    "        # Average validation loss\n",
    "        valid_loss /= len(loaders['valid'].dataset)\n",
    "        \n",
    "        # Print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'\n",
    "              .format(epoch, train_loss, valid_loss))\n",
    "        \n",
    "        # Save the model if it's better\n",
    "        if valid_loss_min > valid_loss:\n",
    "            save(model.state_dict(), save_path)\n",
    "            print('Model saved')\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZjdGHUESjYN"
   },
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XsF22jgx2F4b",
    "outputId": "e0832d1b-dda2-4e7b-ed65-ee218ad12922"
   },
   "outputs": [],
   "source": [
    "epochs = 100 # We will track overfitting during training\n",
    "model_path = 'model.pt'\n",
    "model = train(epochs, dataloaders, model, optimizer, criterion, use_cuda, \n",
    "              model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsQlsWCn6gY_"
   },
   "source": [
    "**Now let's test the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "Zv-4ir9s9xSe",
    "outputId": "ea670931-4704-4556-adb7-f897b99dbfcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import load, device\n",
    "\n",
    "# Check location for model\n",
    "if use_cuda:\n",
    "    location = lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    location = 'cpu'\n",
    "\n",
    "# Load the model parameters to the device used\n",
    "model.load_state_dict(load(model_path, map_location=location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWXkT6QlTUsp"
   },
   "source": [
    "Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPufrpRw9xS0"
   },
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "    # Monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # Move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # Update average test loss\n",
    "        test_loss += ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # Convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # Compare predictions to true label\n",
    "        correct += \\\n",
    "            np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObwRkd8RTjr8"
   },
   "source": [
    "Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "colab_type": "code",
    "id": "vIuZgFun9xTA",
    "outputId": "64217b92-b024-46a5-fb59-114af41614a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.064187\n",
      "\n",
      "\n",
      "Test Accuracy: 97% (4868/4983)\n"
     ]
    }
   ],
   "source": [
    "test(dataloaders, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPtaCU7wZNIi"
   },
   "source": [
    "The model showed a high rate of accuracy on the test sample. \n",
    "However, the dataset requires a deeper cleanup using clustering algorithms to eliminate the remaining problems. \n",
    "Optimization of the model can be aimed at reducing the depth of the network and using the ensemble of networks. \n",
    "In one of the grayscale tests, the model also performed well. \n",
    "Thus, using a less resource-intensive architecture is possible."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sex_detector.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
